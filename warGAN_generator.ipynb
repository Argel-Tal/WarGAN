{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN IMPLEMENTATION FROM COMMUNITY INQ28 MODELS\n",
    "- Code Author: Jack MacCormick\n",
    "- Instagram: argelpaints: https://www.instagram.com/argelpaints/\n",
    "- Done in partnership with 28MAG: https://28-mag.com/\n",
    "\n",
    "**GENERATOR CODE:**\n",
    "\n",
    "This code should allow for the generation of new images, if the .h5 model file is present in the same directory. This is a way to get images, without having to open and run the entire training process locally. The .h5 file is the lastest form of the AI's interpretation of what the truth - of the media it's been fed - is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pylab\n",
    "import h5py\n",
    "\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import random\n",
    "from numpy.random import randint\n",
    "\n",
    "import random\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tqdm import tqdm \n",
    "from IPython import display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default values of standardised images\n",
    "preferred_width = 200\n",
    "preferred_height = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = np.random.uniform(low=0.0, high=1.0, size=(latent_dim, n_samples))\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tx_input = abs(generate_latent_points(latent_dim, n_samples))\n",
    "\t# predict outputs\n",
    "\tX = generator.predict(x_input)\n",
    "\t# create class labels\n",
    "\ty = zeros((n_samples, 1))\n",
    "\treturn X, y"
   ]
  },
  {
   "source": [
    "**APPROACH ONE**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "getting the model to draw something\n",
    "done seperately from the training, as this is relying on pre assembled model representation (.h5 file)\n",
    "\"\"\"\n",
    "\n",
    "latent_dim = 50\n",
    "latent_points = generate_latent_points(latent_dim, 1)\n",
    "# load model in from local dir\n",
    "model = load_model('WARgenerator.h5', compile = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images\n",
    "latent_points = generate_latent_points(latent_dim, 1)\n",
    "predictionInstance = model.predict(latent_points) \n",
    " # convert floats from ranges 0:1 to 0:255, so they can be blackness values in an image\n",
    "xout = predictionInstance*255\n",
    "# corrects array to have the right dimensions\n",
    "flatXOut = xout.flatten()  \n",
    "arr_2d = np.reshape(flatXOut, (200, 200))\n",
    "# convert float array to ints\n",
    "array_int = abs(np.array(arr_2d, dtype='int')) \n",
    "\n",
    "img = Image.fromarray(array_int)\n",
    "img.show()\n",
    "imgRGB = img.convert(mode=\"RGB\")\n",
    "imgRGB.save(\"C:\\\\Users\\\\DataSci\\\\Desktop\\\\DS_Testing\\\\warGAN\\\\output\\\\dream\"+str(0000)+\".jpg\")\n",
    "print(\"done thinking\")"
   ]
  },
  {
   "source": [
    "**APPROACH TWO**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "getting the model to draw something\n",
    "done seperately from the training, as this is relying on pre assembled model representation (.h5 file)\n",
    "\"\"\"\n",
    "\n",
    "latent_dim = 50\n",
    "latent_points = generate_latent_points(latent_dim, 1)\n",
    "# load model in from local dir\n",
    "model = load_model('WARgenerator.h5', compile = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_linear(array, new_min, new_max):\n",
    "    \"\"\"Rescale an arrary linearly.\"\"\"\n",
    "    minimum, maximum = np.min(array), np.max(array)\n",
    "    m = (new_max - new_min) / (maximum - minimum)\n",
    "    b = new_min - m * minimum\n",
    "    return m * array + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate images\n",
    "counter = 1\n",
    "for counter in range(1):\n",
    "    latent_points = generate_latent_points(latent_dim, 1)\n",
    "    predictionInstance = model.predict(latent_points)\n",
    "    # corrects array to have the right dimensions\n",
    "    flatXOut = predictionInstance.flatten()\n",
    "    # correct the range of the images from (-1:1) to (0:255)\n",
    "    flatScale = rescale_linear(flatXOut, 0, 255) # makes them very smooth, much less crisp, which is weird. Also doesn't break when range is outside of (0:255), which should break images\n",
    "    arr_2d = np.reshape(flatScale, (200, 200))    \n",
    "\n",
    "    # convert float array to ints\n",
    "    array_int = (np.array(arr_2d, dtype='int')) \n",
    "\n",
    "    img = Image.fromarray(array_int)\n",
    "    img.show()\n",
    "    imgRGB = img.convert(mode=\"RGB\")\n",
    "    imgRGB.save(\"C:\\\\Users\\\\DataSci\\\\Desktop\\\\DS_Testing\\\\warGAN\\\\output\\\\dream\"+str(00000)+\".jpg\")\n",
    "print(\"done thinking\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('cosc343': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1f4075448429a309b7d91bf840e37bc59aaf1106d5f185478183d34825a04e93"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}